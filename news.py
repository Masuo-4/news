import feedparser
import requests
from bs4 import BeautifulSoup

RSS_URL = "https://news.yahoo.co.jp/rss/topics/business.xml"

def extract_yahoo_full_text(url: str) -> str:
    try:
        res = requests.get(url, timeout=5)
        soup = BeautifulSoup(res.text, "html.parser")
        article_body = soup.select_one("article")

        if not article_body:
            return ""

        paragraphs = []
        for tag in article_body.children:
            if tag.name == "p":
                text = tag.get_text(strip=True)
                if text:
                    paragraphs.append(text)
            elif tag.name in {"div", "section"}:
                break  # é–¢é€£è¨˜äº‹ã‚„PRé ˜åŸŸã§çµ‚äº†

        return "\n".join(paragraphs)

    except Exception as e:
        return f"âš ï¸ Yahooæœ¬æ–‡å–å¾—å¤±æ•—: {e}"

def extract_external_full_text(url: str) -> str:
    try:
        res = requests.get(url, timeout=5)
        soup = BeautifulSoup(res.text, "html.parser")
        raw_paragraphs = soup.find_all("p")

        valid_lines = []
        for p in raw_paragraphs:
            text = p.get_text().strip()
            if text:
                valid_lines.append(text)

        # æœ€å¾Œã® "Copyright" ãªã©ã‚’æ¤œå‡ºã—ã¦ãã“ã‹ã‚‰10æ®µè½åˆ†é™¤å»
        cut_index = None
        for i in reversed(range(len(valid_lines))):
            last_line = valid_lines[i]
            if (
                last_line.startswith("Copyright") or
                last_line.startswith("Copyright Â©") or
                last_line.endswith("ç„¡æ–­è»¢è¼‰ã‚’ç¦ã˜ã¾ã™ã€‚") or
                last_line.endswith("ç„¡æ–­è»¢è¼‰ã‚’ç¦ã˜ã¾ã™")
            ):
                cut_index = i
                break

        if cut_index is not None:
            keep_up_to = max(cut_index - 10, 0)
            valid_lines = valid_lines[:keep_up_to]

        return "\n".join(valid_lines)

    except Exception as e:
        return f"âš ï¸ å¤–éƒ¨è¨˜äº‹å–å¾—å¤±æ•—: {e}"

def fetch_yahoo_full_articles(max_items=5):
    feed = feedparser.parse(RSS_URL)

    if not feed.entries:
        print("âŒ RSSã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return

    for i, entry in enumerate(feed.entries[:max_items], 1):
        print(f"{i}. ğŸ“° {entry.title}")
        print(f"   ğŸ“ Yahoo URL: {entry.link}")

        try:
            res = requests.get(entry.link, timeout=5)
            soup = BeautifulSoup(res.text, "html.parser")

            # ã€Œè¨˜äº‹å…¨æ–‡ã‚’èª­ã‚€ã€ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            external_link = soup.find("a", string="è¨˜äº‹å…¨æ–‡ã‚’èª­ã‚€")

            if external_link and external_link.has_attr("href"):
                full_url = external_link["href"]
                print(f"   ğŸ”— å¤–éƒ¨å…¨æ–‡ãƒªãƒ³ã‚¯: {full_url}")
                external_text = extract_external_full_text(full_url)
                print(f"   ğŸ“– æœ¬æ–‡æŠœç²‹ï¼ˆå¤–éƒ¨ï¼‰:\n{external_text}...\n")
            else:
                print("   â„¹ï¸ å¤–éƒ¨è¨˜äº‹ãƒªãƒ³ã‚¯ãŒãªã„ãŸã‚ã€Yahooæœ¬æ–‡ã‚’æŠ½å‡ºã—ã¾ã™")
                yahoo_text = extract_yahoo_full_text(entry.link)
                if yahoo_text:
                    print(f"   ğŸ“– æœ¬æ–‡æŠœç²‹ï¼ˆYahooå†…ï¼‰:\n{yahoo_text}...\n")
                else:
                    print("   âš ï¸ Yahooæœ¬æ–‡ã‚‚å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

        except Exception as e:
            print(f"   âš ï¸ å…¨ä½“å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        print("-" * 100)

if __name__ == "__main__":
    fetch_yahoo_full_articles()
