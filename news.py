import os
import feedparser
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from dotenv import load_dotenv

from pydantic import BaseModel
from pydantic_ai import Agent
from pydantic_ai.models.gemini import GeminiModel
import asyncio

# =======================
# è¨­å®šãƒ»åˆæœŸåŒ–
# =======================
load_dotenv()
RSS_URL = "https://news.yahoo.co.jp/rss/topics/business.xml"

class Summary(BaseModel):
    filtered_text: str

model = GeminiModel("gemini-2.5-flash")
agent = Agent(model=model, output_type=Summary)

# =======================
# Geminiã§ã‚¿ã‚¤ãƒˆãƒ«é–¢é€£ã®å†…å®¹ã ã‘ã‚’æŠ½å‡º
# =======================
async def extract_title_related_content(title: str, full_text: str) -> str:
    prompt = f"""
ä»¥ä¸‹ã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®å…¨æ–‡ã§ã™ã€‚ãã®ä¸­ã‹ã‚‰ã€Œ{title}ã€ã¨ã„ã†ã‚¿ã‚¤ãƒˆãƒ«ã«é–¢ä¿‚ãªã„éƒ¨åˆ†ã‚’å‰Šé™¤ã—ã¦è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚
ä»–ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦‹å‡ºã—ã€é–¢é€£æƒ…å ±ã€åºƒå‘Šãƒ»è‘—ä½œæ¨©æƒ…å ±ãªã©ã¯é™¤å¤–ã—ã¦ãã ã•ã„ã€‚ã‚¿ã‚¤ãƒˆãƒ«ã«é–¢ä¿‚ã‚ã‚‹éƒ¨åˆ†ã¯çœç•¥ã›ãšã«è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚

å…¨æ–‡:
{full_text}
"""
    result = await agent.run(prompt)
    return result.output.filtered_text

# =======================
# Yahooè¨˜äº‹æŠ½å‡º
# =======================
def extract_yahoo_full_text(url: str) -> str:
    try:
        res = requests.get(url, timeout=5)
        soup = BeautifulSoup(res.text, "html.parser")
        article_body = soup.select_one("article")

        if not article_body:
            return ""

        paragraphs = []
        for tag in article_body.children:
            if tag.name == "p":
                text = tag.get_text(strip=True)
                if text:
                    paragraphs.append(text)
            elif tag.name in {"div", "section"}:
                break  # é–¢é€£è¨˜äº‹ãªã©ã§çµ‚äº†

        return "\n".join(paragraphs)
    except Exception as e:
        return f"âš ï¸ Yahooæœ¬æ–‡å–å¾—å¤±æ•—: {e}"

# =======================
# å¤–éƒ¨å…¨æ–‡æŠ½å‡ºï¼ˆ?page=2ã«ã‚‚å¯¾å¿œã€è‘—ä½œæ¨©æœ«å°¾10æ®µè½ã‚«ãƒƒãƒˆï¼‰
# =======================
def extract_external_full_text(url: str, max_pages: int = 5) -> str:
    try:
        all_paragraphs = []
        parsed_url = urlparse(url)
        base_query = parse_qs(parsed_url.query)

        for page_num in range(1, max_pages + 1):
            query = base_query.copy()
            if page_num > 1:
                query["page"] = [str(page_num)]
            query_str = urlencode(query, doseq=True)
            page_url = urlunparse(parsed_url._replace(query=query_str))

            res = requests.get(page_url, timeout=5)
            if res.status_code >= 400:
                break

            soup = BeautifulSoup(res.text, "html.parser")
            paragraphs = soup.find_all("p")
            if not paragraphs:
                break

            for p in paragraphs:
                text = p.get_text(strip=True)
                if text:
                    all_paragraphs.append(text)

        # è‘—ä½œæ¨©è¡¨è¨˜ã®ã‚«ãƒƒãƒˆ
        cut_index = None
        for i in reversed(range(len(all_paragraphs))):
            last_line = all_paragraphs[i]
            if (
                last_line.startswith("Copyright") or
                last_line.startswith("Copyright Â©") or
                last_line.endswith("ç„¡æ–­è»¢è¼‰ã‚’ç¦ã˜ã¾ã™") or
                last_line.endswith("ç„¡æ–­è»¢è¼‰ã‚’ç¦ã˜ã¾ã™ã€‚")
            ):
                cut_index = i
                break

        if cut_index is not None:
            keep_up_to = max(cut_index - 10, 0)
            all_paragraphs = all_paragraphs[:keep_up_to]

        return "\n".join(all_paragraphs)

    except Exception as e:
        return f"âš ï¸ å¤–éƒ¨è¨˜äº‹å–å¾—å¤±æ•—: {e}"

# =======================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =======================
async def fetch_yahoo_full_articles(max_items=4):
    feed = feedparser.parse(RSS_URL)

    if not feed.entries:
        print("âŒ RSSã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return

    for i, entry in enumerate(feed.entries[:max_items], 1):
        print(f"{i}. ğŸ“° {entry.title}")
        print(f"   ğŸ“ Yahoo URL: {entry.link}")

        try:
            res = requests.get(entry.link, timeout=5)
            soup = BeautifulSoup(res.text, "html.parser")
            external_link = soup.find("a", string="è¨˜äº‹å…¨æ–‡ã‚’èª­ã‚€")

            if external_link and external_link.has_attr("href"):
                full_url = external_link["href"]
                print(f"   ğŸ”— å¤–éƒ¨å…¨æ–‡ãƒªãƒ³ã‚¯: {full_url}")
                external_text = extract_external_full_text(full_url)
                related_text = await extract_title_related_content(entry.title, external_text)
                print(f"ğŸ¯ ã‚¿ã‚¤ãƒˆãƒ«ã«é–¢ä¿‚ã™ã‚‹å†…å®¹ï¼ˆå¤–éƒ¨ï¼‰:\n{related_text}\n")
            else:
                print("   â„¹ï¸ å¤–éƒ¨è¨˜äº‹ãƒªãƒ³ã‚¯ãŒãªã„ãŸã‚ã€Yahooæœ¬æ–‡ã‚’æŠ½å‡ºã—ã¾ã™")
                yahoo_text = extract_yahoo_full_text(entry.link)
                related_text = await extract_title_related_content(entry.title, yahoo_text)
                print(f"ğŸ¯ ã‚¿ã‚¤ãƒˆãƒ«ã«é–¢ä¿‚ã™ã‚‹å†…å®¹ï¼ˆYahooï¼‰:\n{related_text}\n")

        except Exception as e:
            print(f"   âš ï¸ å…¨ä½“å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        print("-" * 100)

if __name__ == "__main__":
    asyncio.run(fetch_yahoo_full_articles())
